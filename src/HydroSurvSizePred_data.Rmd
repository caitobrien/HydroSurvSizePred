---
title: "HydroSurvSizePred_Data"
output: html_document
date: "2024-02-01"
---

```{r packages}
library(tidyverse) #data wrangling
library(here) #file path designation
```

# Size
Data includes detection data of spring/summer Chinook: 

- t_rear_type: H-hatchery, W-wild, U-unknown

- t_run:  1- spring, 2-summer, 5-unknown

- pass_type: T-tranport, ROR-run-of-river; redesignated DART codes(see code below); removed code: S-Sample and 99-Bad Barge Trip

- smolt migration years: all

- smolt migration doy: 80-160

- LWG length: must have length, between 55 - 165

- last detections: LWG-Lower Granite Dam and BON- Bonneville Dam last detections


```{r import_fish_data}
df.fish.raw<-read.csv(here::here("data", "LWG2AdultGrowth.csv"))

df<-df.fish.raw %>% 
  filter(!(is.na(lwg_last) & is.na(bon_last))) %>%  #confirm last detection at LWG or BON
  filter(between(lwg_length, 55, 165) |
           between(bon_length, 55, 165)) %>% #filter lengths 55:165
   mutate(across(c(lwg_last, bon_last), lubridate::ymd, .names = NULL),
          year = lubridate::year(lwg_last),
          doy = lubridate::yday(lwg_last)) %>% 
  filter(between(doy, 80, 160)) %>% #filter doy, 80:160
  # redesignate tranportation codes-- ROR = ROR and SB, SD, SBD, SPD, TB, TBD switched to ROR, 99 and S removed, only T = T
  mutate(pass_type_T_R = case_when(
    str_detect(pass_type, "S") & str_length(pass_type) > 1 ~ "ROR",
    str_detect(pass_type, "S") & str_length(pass_type) == 1 ~ "remove",
    str_detect(pass_type, "TB") ~ "ROR",
    str_detect(pass_type, "TBD") ~ "ROR",
    str_detect(pass_type, "ROR") ~ "ROR",
    str_detect(pass_type, "T") ~ "T",
    .default = "remove"
  )) %>%
  filter(pass_type_T_R != "remove") %>% 
  pivot_longer(cols = c(lwg_length, bon_length), names_to = "site", values_to = "length", values_drop_na = TRUE) %>% 
  mutate(site = toupper(str_remove(site, "_length"))) %>% 
  filter(year == c(1993:2020)) %>% 
  #add levels for additional facetting used in the smoltsize histogram subpage1
   mutate(by_month = lubridate::month(lwg_last, label = TRUE, abbr = FALSE),
         day_month = lubridate::day(lwg_last)) %>% 
  arrange(by_month, day_month) %>%  # Arrange the dataframe
  mutate(by_half_month = factor(ifelse(day_month <= 15, paste0("Early ", by_month), paste0("Late ", by_month)), levels = c("Late March","Early April", "Late April", "Early May", "Late May", "Early June"), ordered = TRUE)) %>% #need a better way to do this
  mutate(across(c(year, doy, length), as.numeric))


readr::write_csv(df, here::here("data", "spsuCH_subset.csv"))

```

```{r fish_sums}

summary_data <- df_fish %>%
  group_by(year, by_month, by_half_month) %>%
  summarise(total_smolt_BON = sum(site == "BON", na.rm = TRUE),
            total_smolt_LWG = sum(site == "LWG", na.rm = TRUE)) %>%
  ungroup()

max(df_fish$length)


   # Create a summary data frame that contains the total number of smolt per site per year, month, or half-month
   summary_data <- df_fish %>%
     group_by(year, site) %>%
     summarise(nsite = sum(n(), na.rm = TRUE)) %>%
     ungroup()

   # Spread the counts into separate columns for each site
   summary_data <- tidyr::spread(summary_data, key = "site", value = "nsite")

   # Replace NA values with 0
   summary_data[is.na(summary_data)] <- 0

   # Gather the counts back into a single column with the corresponding site names
   summary_data <- tidyr::gather(summary_data, key = "site", value = "nsite", c("BON", "LWG"))

   # Create the label
   summary_data <- summary_data %>%
     mutate(label = paste(site, "n= ", nsite))

   # Combine the labels for "BON" and "LWG"
   summary_data <- summary_data %>%
     group_by(year) %>%
     summarise(label = paste(label, collapse = "\n")) %>%
     ungroup()
```



```{r predator_thresholds}
predator_thresholds<-tibble(species = c("N. Pikeminnow", "Pacific Hake"),
                                median = c(166.165, 110),
                                min = c(76.665, 70),
                                max = c(255.165, 200))

pred_long<-predator_thresholds %>% 
  pivot_longer(cols = c(median, min, max), values_to = "threshold", names_to = "type") %>% 
  mutate(type = ifelse(type != "median","min/max", "median"),
         predator = species)

```



# Predation


## Pacific Hake
**methods from shiny app:** 

IMPLENTING EMMETT 2008 – PACIFIC HAKE DATA
Emmett 2008, page 5, boxplot image was inserted into https://plotdigitizer.com/app and the five-summary data was extracted from 1998-2004, 2017, and 2019 and saved into “Pacific Hake Data.csv”.

Data was then converted from predator length to prey length utilizing y~0.350838x-33.3482 to apply the linear relationship to determine prey size susceptible to predation (Muir et al 2006, page 1526, for Pacific Hake). [Pacific Hake Conversion.R]

The five-summary data was then graphed utilizing lines of dark green where low & high were dotted lines, q1 & q3 were dashed lines, and median was a solid line. 

&

CLIMATE YEARS
While we were able to get data for predators in a few years to determine their thresholds, a large majority of years did not have such collected data. Thus, using Oceanic Niño Index (ONI) and Pacific Decadal Oscillation (PDO), we categorized both dataset values into negative, neutral, and positive. If a year had positive values from both datasets, the year would be labeled as “positive,” vice versa for “negative.” If the year had differing values, it would be labeled as “neutral.”

From there, we took the threshold data that was available to us and separated them based on their year’s climate and then averaged them to determine a “default” threshold for each of the three climates.

```{r import_hake_pred_data}

df.pred.hake.raw<-read_csv(here::here("data/predation_risk/hake/year_data", "1998-2004 Hake Data.csv"))  

#adjust 2002 to be correct
df.pred.hake.raw[5,1] <- 2002
df.pred.hake.raw %>% 
  mutate(year = as.factor(year))
# data from 1998 to 2004 pulled data from Emmett et al 2011; 2017 and 2019 pulled from ? 
#multiple files for 2017 & 2019, if using 2017/2019 values in `Hake Data (All).csv` then SL for 2017/2019 are negative, but final summary data reports values for 2017, 2015, 2019 and none are negative. What was done there? 



```
```{r convert_predlengths_to_preylengths}


# Define the equation: y ~ 0.350838x - 33.3482 (Muir et al 2006)
hake_convert_y <- function(x) {
  return(0.350838 * x - 33.3482)
}


# Apply the equation to each non-year column of the dataset
hake_results_df <- df.pred.hake.raw %>%
  mutate_at(vars(-year), ~ hake_convert_y(.)) %>% 
  mutate(year = as.factor(year))

# Save the SL results to a CSV file
write.csv(hake_results_df, file = here::here("data/predation_risk/hake/", "hake_predtoprey_SL_results.csv"), row.names = FALSE)



#plot explained in methods

#including 2017/2019
ggplot(hake_results_df, aes(x = year))+
  geom_point(aes(y= median)) +
  geom_line(aes(y= median), linetype = "solid") +
  geom_point(aes(y= min)) +
  geom_line(aes(y= min), linetype = "dotted") +
    geom_point(aes(y= max)) +
  geom_line(aes(y= max), linetype = "dotted") +
    geom_point(aes(y= q1)) +
  geom_line(aes(y= q1), linetype = "dashed") +
      geom_point(aes(y= q3)) +
  geom_line(aes(y= q3), linetype = "dashed") +
  geom_hline(yintercept = 0)


#removing 2017:2019
ggplot(filter(hake_results_df, year <2015), aes(x = year))+
  geom_point(aes(y= median)) +
  geom_line(aes(y= median), linetype = "solid") +
  geom_point(aes(y= min)) +
  geom_line(aes(y= min), linetype = "dotted") +
    geom_point(aes(y= max)) +
  geom_line(aes(y= max), linetype = "dotted") +
    geom_point(aes(y= q1)) +
  geom_line(aes(y= q1), linetype = "dashed") +
      geom_point(aes(y= q3)) +
  geom_line(aes(y= q3), linetype = "dashed")
```

adding climate data (Negative, Neutral, Positive) to predict all years thresholds
*no positive year data to get median ---rethink method all together

```{r import_climate_data}
#add automated version scrubbing from website

#pull files for now
df_climate<-read.csv(file = here::here("data/predation_risk", "ONI_PDO Data.csv"))


#assign climate designation per year based on pdo/oni classification
df_climate <-df_climate %>% 
  filter(between(year, 1993, 2022)) %>% 
    mutate(
    pdo.oni_group = ifelse(pdo_group == oni_group, pdo_group, "neutral")
  ) %>% 
  mutate(year= as.factor(year))
```


```{r combine_climate&hake_predict_data}


#expand known data to add estimation based on climate designation
expanded_df <- hake_results_df %>%
  full_join(df_climate %>% select(year, pdo.oni_group), by = "year") 

    
#average thresholds based on climate designation   
expanded_averages_df<-expanded_df %>% 
  group_by(pdo.oni_group) %>% 
   mutate_at(vars(-year, -pdo.oni_group), ~ mean(., na.rm = TRUE)) 

#*no average for positive years based on available data
#*data shared has diff values for years with data, but don't reflect average used in predicted data. __REDO METHOD AND DATA SOURCING

```

To add pike threshold data: 

**methods from shiny app:** 
GETTING NORTHERN PIKE THRESHOLD
For the Northern Pikeminnow length distributions (ranges & medians), we utilized Figure 10 in Winther et al. 2021 and Winther et al. 2019, Table D-1 in Storch et al. 2014, and Figure 10 in Porter et al. 2013.

For each figure, we obtained the min and max values as well as the median range. From there, we took the median of the median range and used that as the median threshold.

Median of each bin, weight by the percentage observed, sum across bins => average.

Since all the graphs had relatively the same shape and similar averages, the thresholds are set static across all years to min = 76.665, median = 166.165, and max = 255.165 after using the Muir equation for N. Pikeminnows.

*for now just use data and then rethink overall method and source from each report listed


```{r calc_pct_susceptible}

df_fish<-read.csv(here::here("data/size_distribution", "spsuCH_subset.csv")) #subsetted data from lWG2AdultGrowth.csv; see size code
#see past methods description--setting pike set median -- rethink method
pike.median<-166.165

#combine climate and hake df; get count under pred threshold
df_pred_hake <- df_fish %>% 
  mutate( year = as.factor(year)) %>% 
  left_join(select(expanded_averages_df, year, median, pdo.oni_group), by = "year") %>% 
  mutate(prey = ifelse(length <= median, 1,0))

#calc pct suceptible hake
df_pred_summary_hake <- df_pred_hake %>% 
  group_by(year, site) %>% 
  summarise(n_susceptible = sum(prey),
            n_population = n(),
            pct_susceptible = round((n_susceptible / n_population),4)) %>% 
  mutate(predator = "Pacific Hake",
         year = as.character(year))

#designate count under pike pred threshold
df_pred_pike<- df_fish %>% 
  mutate(prey = ifelse(length <= pike.median, 1,0))  #all are going tobe prey since the df_fish is filtered from 55 to 165!

#calc pct suceptible pike
df_pred_summary_pike <- df_pred_pike %>% 
  group_by(year, site) %>% 
  summarise(n_susceptible = sum(prey),
            n_population = n(),
            pct_susceptible = round((n_susceptible / n_population),4)) %>% 
  mutate(predator = "N. Pikeminnow", 
         year = as.character(year))  

#combine summaries
df_pred_summary<-rbind(df_pred_summary_hake, df_pred_summary_pike)
#*positive years missing obviously--update when method selected

write_csv(df_pred_summary, here::here("data/predation_risk", "predtopreySL_to_pctsusceptible.csv"))

```

# Survival

**methods from shiny app:** 

```{r shared_surv_code}
# Description: Reads all the migration-survival files and prepare data to be 
# graphed. 

# LGR_BON MIGRATION #
#####################
lgr_bon_r <- read.csv(here::here("data/survival/results", "LGR_BON_R_surv.csv"), header = FALSE, sep = ",")

# Transpose columns 
lgr_bon_r <- as.data.frame(t(lgr_bon_r))
# Rename columns 
lgr_bon_r <- lgr_bon_r %>% rename("year" = "V1", "min" = "V2", "median" = "V3", "max" = "V4")
# Convert from decimal to percentage
lgr_bon_r[, 2:4] <- lapply(lgr_bon_r[, 2:4], as.numeric)

lgr_bon_r[2:4] = lapply(lgr_bon_r[2:4], "*", 100)

# Create transfer dataframe where survival is assumed 100%
lgr_bon_t <- lgr_bon_r %>% mutate_at(vars(min, median, max), list(~100))
lgr_bon_t[, 2:4] <- lapply(lgr_bon_t[, 2:4], as.numeric)

# Add 'which dataframe' label
lgr_bon_r <- lgr_bon_r %>% add_column(migration = "In-river")
lgr_bon_t <- lgr_bon_t %>% add_column(migration = "Transported")

# Merge into LGR_BON DF
lgr_bon_df <- rbind(lgr_bon_r,lgr_bon_t) 

#Add column designating reach specific for combined df
lgr_bon_df<-lgr_bon_df %>% 
  mutate(reach = "LGR_BON")

# BON_BOA MIGRATION #
#####################
bon_boa_r <- read.csv(here::here("data/survival/results", "BON_BOA_R_surv.csv"), header = FALSE, sep = ",")
bon_boa_t <- read.csv(here::here("data/survival/results", "BON_BOA_T_surv.csv"), header = FALSE, sep = ",")
# Transpose columns 
bon_boa_r <- as.data.frame(t(bon_boa_r))
bon_boa_t <- as.data.frame(t(bon_boa_t))
# Rename columns 
bon_boa_r <- bon_boa_r %>% rename("year" = "V1", "min" = "V2", "median" = "V3", "max" = "V4")
bon_boa_t <- bon_boa_t %>% rename("year" = "V1", "min" = "V2", "median" = "V3", "max" = "V4")
# Convert from decimal to percentage
bon_boa_r[, 2:4] <- lapply(bon_boa_r[, 2:4], as.numeric)
bon_boa_t[, 2:4] <- lapply(bon_boa_t[, 2:4], as.numeric)
bon_boa_r[2:4] = lapply(bon_boa_r[2:4], "*", 100)
bon_boa_t[2:4] = lapply(bon_boa_t[2:4], "*", 100)
# Add 'which dataframe' label
bon_boa_r <- bon_boa_r %>% add_column(migration = "In-river")
bon_boa_t <- bon_boa_t %>% add_column(migration = "Transported")

# Merge into LGR_BON DF
bon_boa_df <- rbind(bon_boa_r,bon_boa_t)

#Add column designating reach specific for combined df
bon_boa_df<-bon_boa_df %>% 
  mutate(reach = "BON_BOA")

# LGR_BOA MIGRATION #
#####################
lgr_boa_r <- read.csv(here::here("data/survival/results", "LGR_BOA_R_surv.csv"), header = FALSE, sep = ",")
lgr_boa_t <- read.csv(here::here("data/survival/results", "LGR_BOA_T_surv.csv"), header = FALSE, sep = ",")
# Transpose columns 
lgr_boa_r <- as.data.frame(t(lgr_boa_r))
lgr_boa_t <- as.data.frame(t(lgr_boa_t))
# Rename columns 
lgr_boa_r <- lgr_boa_r %>% rename("year" = "V1", "min" = "V2", "median" = "V3", "max" = "V4")
lgr_boa_t <- lgr_boa_t %>% rename("year" = "V1", "min" = "V2", "median" = "V3", "max" = "V4")
# Convert from decimal to percentage
lgr_boa_r[, 2:4] <- lapply(lgr_boa_r[, 2:4], as.numeric)
lgr_boa_t[, 2:4] <- lapply(lgr_boa_t[, 2:4], as.numeric)
lgr_boa_r[2:4] = lapply(lgr_boa_r[2:4], "*", 100)
lgr_boa_t[2:4] = lapply(lgr_boa_t[2:4], "*", 100)
# Add 'which dataframe' label
lgr_boa_r <- lgr_boa_r %>% add_column(migration = "In-river")
lgr_boa_t <- lgr_boa_t %>% add_column(migration = "Transported")

# Merge into LGR_BON DF
lgr_boa_df <- rbind(lgr_boa_r,lgr_boa_t)
#Add column designating reach specific for combined df
lgr_boa_df<-lgr_boa_df %>% 
  mutate(reach = "LGR_BOA")

# DATAFRAME MERGING #
#####################
# Merge all migration dataframes 
#survival_df <- rbind(lgr_bon_r,lgr_bon_t,bon_boa_r,bon_boa_t,lgr_boa_r,lgr_boa_t)

survival_df <- rbind(lgr_bon_df,bon_boa_df,lgr_boa_df)


# Jittering (what is this doing?)
lgr_bon_df$year_adj <- ifelse(lgr_bon_df$migration == "ROR", as.numeric(lgr_bon_df$year) - 0.1, as.numeric(lgr_bon_df$year) + 0.1)
bon_boa_df$year_adj <- ifelse(bon_boa_df$migration == "ROR", as.numeric(bon_boa_df$year) - 0.1, as.numeric(bon_boa_df$year) + 0.1)
lgr_boa_df$year_adj <- ifelse(lgr_boa_df$migration == "ROR", as.numeric(lgr_boa_df$year) - 0.1, as.numeric(lgr_boa_df$year) + 0.1)


survival_df$year<- round(as.numeric(survival_df$year))

write.csv(survival_df,here::here("data/survival", "cjs_reach_survival.csv"))
```

```{r surv_data_visualizations}

df_survival<-read.csv(here::here("data/survival", "cjs_reach_survival.csv"))

ggplot(df_survival)+
  geom_point(aes(x= year, y= median, color = migration)) +
  geom_line( aes(x=year, y=median,  color = migration)) + 
  geom_point(data = df_pred_summary, aes(x=as.numeric(year), y=pct_susceptible, fill = predator), shape = 21)+
  facet_wrap(~reach, ncol = 1, scale = "free")

as.integer(as.numeric(df_pred$year))
```

# Summary

```{r}
predator_thresholds 


  #create base plot
   size_plot <- df_fish %>%
     filter(year == 2013) %>% 
    ggplot(aes(x = length)) +
    geom_histogram(aes(fill = site), binwidth = 5) +
    geom_vline(data = filter(pred_long,  type == "median"), aes(color = species, xintercept = threshold, linetype = type )) +
    scale_fill_manual(values = c("LWG" = "steelblue4", "BON" = "#b47747"),
                      labels = c("LWG", "BON")) +
    scale_color_manual(values = c("N. Pikeminnow" = "darkgreen", "Pacific Hake" = "goldenrod"),
                 labels = c("N. Pikeminnow", "Pacific Hake")) +
    labs(
      x = "Fork length (mm)",
      y= "Number of smolt",
      fill = "Location",
      color = "Predator median",
      linetype = NULL) +
     guides(linetype = FALSE, 
            color = FALSE) + 
     facet_wrap(~year)
    
   
   
   pred_plot <- df_pred_summary %>% 
     filter(year == 2013) %>% 
     ggplot(aes(x=pct_susceptible, y=site, fill = predator)) +
     geom_bar(stat= "identity", position = position_dodge()) +
    scale_fill_manual (values = c("N. Pikeminnow" = "darkgreen", "Pacific Hake" = "goldenrod"),
                 labels = c("N. Pikeminnow", "Pacific Hake")) +
     labs(x = "Predation risk (%)",
          y = "Location") + 
     facet_wrap(~year)
   
   
   surv_plot<- df_survival %>%
     filter(year == 2013, 
            reach == "LGR_BOA") %>% 
    ggplot(aes(x = as.factor(year), y = median/100))+
    geom_pointrange(aes(color= migration,y=median/100, ymin=min/100, ymax=max/100),
                    shape =21, 
                    size = 1, 
                    lwd =.25, 
                    position = position_jitter())+
    labs( y= "Estimated survival (%)",
          x = "Smolt release year",
         # shape = "Predation risk (%)",
          color = "Passage type") +
    scale_y_continuous(labels = scales::percent_format())+
    theme_light() +
     facet_wrap(~year)
    theme(panel.spacing.y = unit(1, "cm"))
   

library(patchwork)   
  
   # Arrange plots with patchwork and add annotation and legend
top_legends <- (
  ((size_plot / pred_plot) | surv_plot ) + 
  plot_annotation(title = "YEAR--reactive") +
  plot_layout(guides = "collect") &
  theme(legend.position='top')
)

# Display the plot
top_legends



```

